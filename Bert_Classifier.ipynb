{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bert_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c46dfc712e44650a7966aa75417191e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_209e4613327549f7b3cb64416fa32dc2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_08ca585929e540829101d95ea1cd3eec",
              "IPY_MODEL_d237f0c6eb744c228eb6017cbd27351e"
            ]
          }
        },
        "209e4613327549f7b3cb64416fa32dc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08ca585929e540829101d95ea1cd3eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cca420cb32544bb296e0f84959a252b0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0850715a2170422885a72222d9c2c860"
          }
        },
        "d237f0c6eb744c228eb6017cbd27351e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dc8cd9be3c55451082d41e7f436d5bc1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 314kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b0e146bb98b449938cbbe0b9913d9923"
          }
        },
        "cca420cb32544bb296e0f84959a252b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0850715a2170422885a72222d9c2c860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc8cd9be3c55451082d41e7f436d5bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b0e146bb98b449938cbbe0b9913d9923": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c137664fcacf40789d4013964884efcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_87c032e76fe64fc0bd71a8ed95642eca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7a94494007ba4444aad8070c0f117590",
              "IPY_MODEL_ae3e2eee00da4bc7b5e7c3c03c3ffe48"
            ]
          }
        },
        "87c032e76fe64fc0bd71a8ed95642eca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a94494007ba4444aad8070c0f117590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d63120b0af34c56a37c7c7a2723b109",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08025bfc30ea411f94ffbef508c5ecb4"
          }
        },
        "ae3e2eee00da4bc7b5e7c3c03c3ffe48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2ac12e3deb144dcca066235a870d289c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 479B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a78e3c6595e9481482b766d98ef134ad"
          }
        },
        "9d63120b0af34c56a37c7c7a2723b109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08025bfc30ea411f94ffbef508c5ecb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ac12e3deb144dcca066235a870d289c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a78e3c6595e9481482b766d98ef134ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "754baf232f95407db7795ec2add4324c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0a10c6b0c1fd48c6aff3099a72b95ea2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_269a742db87c4fa1857925387fde7448",
              "IPY_MODEL_3eca69f189af4b578f36f55086ecdf63"
            ]
          }
        },
        "0a10c6b0c1fd48c6aff3099a72b95ea2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "269a742db87c4fa1857925387fde7448": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b819ee0bc819438d89a409526710103a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fef19a31f4ce45fca724ea186904b8b4"
          }
        },
        "3eca69f189af4b578f36f55086ecdf63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ca52ee516dc4a1189bbef58885139f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:15&lt;00:00, 27.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e75fd39c76c44c0b5663d8586048abe"
          }
        },
        "b819ee0bc819438d89a409526710103a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fef19a31f4ce45fca724ea186904b8b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ca52ee516dc4a1189bbef58885139f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e75fd39c76c44c0b5663d8586048abe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcPJdwHXmlrY",
        "outputId": "1c637304-342e-4453-9f57-d7f2ddc4d301"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 42.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 60.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=0a90c8ecf28ef9755354e5ec097375b186f249b2b1a96b2b928dc487289666a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4qLcS4nxIL"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hLnPpV5pLzJ"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w27MMo011r5q",
        "outputId": "e68fe573-86f5-47a1-e1ab-0fd43ef98f82"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRCKvcB1pmgs"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/training.1600000.processed.noemoticon.csv\", names=['target', 'ids', 'date', 'flag', 'user', 'text'],\n",
        "                encoding='latin-1', error_bad_lines=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "xuQ7vtZRpq98",
        "outputId": "07b7a87d-5020-474d-ad2d-15b9603cb709"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  ...                                               text\n",
              "0       0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1       0  ...  is upset that he can't update his Facebook by ...\n",
              "2       0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3       0  ...    my whole body feels itchy and like its on fire \n",
              "4       0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxhZxeYxsMrQ"
      },
      "source": [
        "down_sample = 100000\n",
        "neg_index = np.random.choice(dataset[dataset.target == 0].index, size=int(.5*down_sample), replace=False)\n",
        "pos_index = np.random.choice(dataset[dataset.target == 4].index, size=int(.5*down_sample), replace=False)\n",
        "red_index = np.concatenate((neg_index, pos_index))\n",
        "data = dataset.iloc[red_index,:]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "oRZPOfr9kUoK",
        "outputId": "3c39b1c2-ce54-496e-e387-7d3610e41ca8"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>196222</th>\n",
              "      <td>0</td>\n",
              "      <td>1970673079</td>\n",
              "      <td>Sat May 30 04:47:42 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>jakibabi</td>\n",
              "      <td>is a really big turnoff when guys swears too m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547371</th>\n",
              "      <td>0</td>\n",
              "      <td>2202058323</td>\n",
              "      <td>Tue Jun 16 21:10:35 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ckellyireland7</td>\n",
              "      <td>@TokioHotel4LYF oh....thats kinda sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763996</th>\n",
              "      <td>0</td>\n",
              "      <td>2298656660</td>\n",
              "      <td>Tue Jun 23 11:57:07 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>JedBurrows</td>\n",
              "      <td>I uh....i think im going to go shave  http://d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460118</th>\n",
              "      <td>0</td>\n",
              "      <td>2072344775</td>\n",
              "      <td>Sun Jun 07 20:50:51 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>khairulinho</td>\n",
              "      <td>sending my idiotic viewsonic monitor for repai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697021</th>\n",
              "      <td>0</td>\n",
              "      <td>2253695416</td>\n",
              "      <td>Sat Jun 20 08:35:43 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>joe_316</td>\n",
              "      <td>http://twitpic.com/7wmfp starting to fill up n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        target  ...                                               text\n",
              "196222       0  ...  is a really big turnoff when guys swears too m...\n",
              "547371       0  ...             @TokioHotel4LYF oh....thats kinda sad \n",
              "763996       0  ...  I uh....i think im going to go shave  http://d...\n",
              "460118       0  ...  sending my idiotic viewsonic monitor for repai...\n",
              "697021       0  ...  http://twitpic.com/7wmfp starting to fill up n...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gr0MWQMp5IE",
        "outputId": "b57b57c6-9bf6-49f0-9747-856eda4e38e5"
      },
      "source": [
        "data.target = data.target.replace({0: 0, 4: 1})\n",
        "data.target.value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    50000\n",
              "0    50000\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg-8ZdwJp91g"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.text.values\n",
        "y = data.target.values\n",
        "\n",
        "X_train, X_val, y_train, y_val =\\\n",
        "    train_test_split(X, y, test_size=0.1, random_state=2020)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9iCQsoArROb"
      },
      "source": [
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgkQ91srreJk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "3c46dfc712e44650a7966aa75417191e",
            "209e4613327549f7b3cb64416fa32dc2",
            "08ca585929e540829101d95ea1cd3eec",
            "d237f0c6eb744c228eb6017cbd27351e",
            "cca420cb32544bb296e0f84959a252b0",
            "0850715a2170422885a72222d9c2c860",
            "dc8cd9be3c55451082d41e7f436d5bc1",
            "b0e146bb98b449938cbbe0b9913d9923"
          ]
        },
        "outputId": "1ab3dc40-0a9b-4955-e88e-b7aac335fc51"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c46dfc712e44650a7966aa75417191e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00D0fFuty_iq",
        "outputId": "eb996970-1d50-4cd3-f406-6a2e52b50b65"
      },
      "source": [
        "import random\n",
        "all_tweets = data.text.values\n",
        "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True, max_length=510) for sent in all_tweets]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1NcFnbW0M9R",
        "outputId": "fd2fb8ee-90d7-4d95-8601-1da541698c62"
      },
      "source": [
        "max_len = max([len(sent) for sent in encoded_tweets])\n",
        "print('Max length: ', max_len)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length:  177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DabdUce30Oo",
        "outputId": "2f81e8c3-ed1c-433a-d07f-9ad7745112e6"
      },
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN = 100\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  is a really big turnoff when guys swears too much \n",
            "Token IDs:  [101, 2003, 1037, 2428, 2502, 2735, 7245, 2043, 4364, 8415, 2015, 2205, 2172, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Tokenizing data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxWyvvJCvI7g",
        "outputId": "1dfce486-3f52-4cb7-fcef-a3f749983135"
      },
      "source": [
        "#import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgtCK8OB-lBK",
        "outputId": "ec39fc59-18d8-4908-e5ed-1a45636d5edc"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "print(\"Shape of train_dataloader\", len(train_dataloader))\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "print(\"Shape of val_dataloader\", len(val_dataloader))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_dataloader 2813\n",
            "Shape of val_dataloader 313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F11o1rbCBiVK",
        "outputId": "fa3e9094-5cd6-42b0-a93e-c2645719a173"
      },
      "source": [
        "%%time\n",
        "#import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        \n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 68 µs, sys: 0 ns, total: 68 µs\n",
            "Wall time: 71 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP2r78mtCojU"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=2e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(\"total_steps\", total_steps)\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=1000, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM4NFERfC7HR"
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    with torch.no_grad():\n",
        "      for batch in val_dataloader:\n",
        "          # Load batch to GPU\n",
        "          b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "          # Compute logits\n",
        "          \n",
        "          logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "          # Compute loss\n",
        "          loss = loss_fn(logits, b_labels)\n",
        "          val_loss.append(loss.item())\n",
        "\n",
        "          # Get the predictions\n",
        "          preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "          # Calculate the accuracy rate\n",
        "          accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "          val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c137664fcacf40789d4013964884efcd",
            "87c032e76fe64fc0bd71a8ed95642eca",
            "7a94494007ba4444aad8070c0f117590",
            "ae3e2eee00da4bc7b5e7c3c03c3ffe48",
            "9d63120b0af34c56a37c7c7a2723b109",
            "08025bfc30ea411f94ffbef508c5ecb4",
            "2ac12e3deb144dcca066235a870d289c",
            "a78e3c6595e9481482b766d98ef134ad",
            "754baf232f95407db7795ec2add4324c",
            "0a10c6b0c1fd48c6aff3099a72b95ea2",
            "269a742db87c4fa1857925387fde7448",
            "3eca69f189af4b578f36f55086ecdf63",
            "b819ee0bc819438d89a409526710103a",
            "fef19a31f4ce45fca724ea186904b8b4",
            "1ca52ee516dc4a1189bbef58885139f4",
            "5e75fd39c76c44c0b5663d8586048abe"
          ]
        },
        "id": "OhukZRn4DKyg",
        "outputId": "ae7415e6-6700-4d23-929f-ec9a3e1bbe6c"
      },
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=4)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=4, evaluation=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c137664fcacf40789d4013964884efcd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "754baf232f95407db7795ec2add4324c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "total_steps 11252\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.707757   |     -      |     -     |   6.88   \n",
            "   1    |   40    |   0.696368   |     -      |     -     |   6.46   \n",
            "   1    |   60    |   0.704308   |     -      |     -     |   6.41   \n",
            "   1    |   80    |   0.692426   |     -      |     -     |   6.38   \n",
            "   1    |   100   |   0.692096   |     -      |     -     |   6.41   \n",
            "   1    |   120   |   0.693039   |     -      |     -     |   6.44   \n",
            "   1    |   140   |   0.680259   |     -      |     -     |   6.39   \n",
            "   1    |   160   |   0.673587   |     -      |     -     |   6.44   \n",
            "   1    |   180   |   0.668272   |     -      |     -     |   6.40   \n",
            "   1    |   200   |   0.644666   |     -      |     -     |   6.43   \n",
            "   1    |   220   |   0.630328   |     -      |     -     |   6.44   \n",
            "   1    |   240   |   0.594020   |     -      |     -     |   6.42   \n",
            "   1    |   260   |   0.573334   |     -      |     -     |   6.45   \n",
            "   1    |   280   |   0.546678   |     -      |     -     |   6.48   \n",
            "   1    |   300   |   0.507463   |     -      |     -     |   6.41   \n",
            "   1    |   320   |   0.532405   |     -      |     -     |   6.41   \n",
            "   1    |   340   |   0.508844   |     -      |     -     |   6.41   \n",
            "   1    |   360   |   0.486340   |     -      |     -     |   6.39   \n",
            "   1    |   380   |   0.447307   |     -      |     -     |   6.40   \n",
            "   1    |   400   |   0.492345   |     -      |     -     |   6.42   \n",
            "   1    |   420   |   0.477046   |     -      |     -     |   6.41   \n",
            "   1    |   440   |   0.462022   |     -      |     -     |   6.46   \n",
            "   1    |   460   |   0.449581   |     -      |     -     |   6.44   \n",
            "   1    |   480   |   0.442657   |     -      |     -     |   6.43   \n",
            "   1    |   500   |   0.432836   |     -      |     -     |   6.47   \n",
            "   1    |   520   |   0.487467   |     -      |     -     |   6.42   \n",
            "   1    |   540   |   0.450899   |     -      |     -     |   6.40   \n",
            "   1    |   560   |   0.451761   |     -      |     -     |   6.46   \n",
            "   1    |   580   |   0.454432   |     -      |     -     |   6.42   \n",
            "   1    |   600   |   0.470001   |     -      |     -     |   6.41   \n",
            "   1    |   620   |   0.443433   |     -      |     -     |   6.42   \n",
            "   1    |   640   |   0.437729   |     -      |     -     |   6.47   \n",
            "   1    |   660   |   0.433153   |     -      |     -     |   6.45   \n",
            "   1    |   680   |   0.470996   |     -      |     -     |   6.41   \n",
            "   1    |   700   |   0.459070   |     -      |     -     |   6.47   \n",
            "   1    |   720   |   0.414791   |     -      |     -     |   6.43   \n",
            "   1    |   740   |   0.414040   |     -      |     -     |   6.44   \n",
            "   1    |   760   |   0.439140   |     -      |     -     |   6.44   \n",
            "   1    |   780   |   0.459153   |     -      |     -     |   6.44   \n",
            "   1    |   800   |   0.428077   |     -      |     -     |   6.42   \n",
            "   1    |   820   |   0.476482   |     -      |     -     |   6.41   \n",
            "   1    |   840   |   0.446183   |     -      |     -     |   6.40   \n",
            "   1    |   860   |   0.432201   |     -      |     -     |   6.49   \n",
            "   1    |   880   |   0.449334   |     -      |     -     |   6.43   \n",
            "   1    |   900   |   0.424323   |     -      |     -     |   6.43   \n",
            "   1    |   920   |   0.422177   |     -      |     -     |   6.50   \n",
            "   1    |   940   |   0.412115   |     -      |     -     |   6.44   \n",
            "   1    |   960   |   0.441860   |     -      |     -     |   6.46   \n",
            "   1    |   980   |   0.438802   |     -      |     -     |   6.43   \n",
            "   1    |  1000   |   0.424340   |     -      |     -     |   6.45   \n",
            "   1    |  1020   |   0.404138   |     -      |     -     |   6.43   \n",
            "   1    |  1040   |   0.424311   |     -      |     -     |   6.46   \n",
            "   1    |  1060   |   0.398933   |     -      |     -     |   6.46   \n",
            "   1    |  1080   |   0.422578   |     -      |     -     |   6.42   \n",
            "   1    |  1100   |   0.441265   |     -      |     -     |   6.47   \n",
            "   1    |  1120   |   0.432280   |     -      |     -     |   6.45   \n",
            "   1    |  1140   |   0.383303   |     -      |     -     |   6.44   \n",
            "   1    |  1160   |   0.360188   |     -      |     -     |   6.46   \n",
            "   1    |  1180   |   0.374447   |     -      |     -     |   6.42   \n",
            "   1    |  1200   |   0.396267   |     -      |     -     |   6.45   \n",
            "   1    |  1220   |   0.419281   |     -      |     -     |   6.46   \n",
            "   1    |  1240   |   0.374181   |     -      |     -     |   6.41   \n",
            "   1    |  1260   |   0.411340   |     -      |     -     |   6.42   \n",
            "   1    |  1280   |   0.404971   |     -      |     -     |   6.42   \n",
            "   1    |  1300   |   0.392164   |     -      |     -     |   6.47   \n",
            "   1    |  1320   |   0.363486   |     -      |     -     |   6.45   \n",
            "   1    |  1340   |   0.389608   |     -      |     -     |   6.48   \n",
            "   1    |  1360   |   0.371602   |     -      |     -     |   6.41   \n",
            "   1    |  1380   |   0.434254   |     -      |     -     |   6.41   \n",
            "   1    |  1400   |   0.383284   |     -      |     -     |   6.46   \n",
            "   1    |  1420   |   0.430434   |     -      |     -     |   6.43   \n",
            "   1    |  1440   |   0.436890   |     -      |     -     |   6.44   \n",
            "   1    |  1460   |   0.393517   |     -      |     -     |   6.45   \n",
            "   1    |  1480   |   0.392244   |     -      |     -     |   6.43   \n",
            "   1    |  1500   |   0.417863   |     -      |     -     |   6.42   \n",
            "   1    |  1520   |   0.392209   |     -      |     -     |   6.42   \n",
            "   1    |  1540   |   0.428402   |     -      |     -     |   6.39   \n",
            "   1    |  1560   |   0.418782   |     -      |     -     |   6.41   \n",
            "   1    |  1580   |   0.420160   |     -      |     -     |   6.42   \n",
            "   1    |  1600   |   0.415364   |     -      |     -     |   6.40   \n",
            "   1    |  1620   |   0.376554   |     -      |     -     |   6.39   \n",
            "   1    |  1640   |   0.425938   |     -      |     -     |   6.43   \n",
            "   1    |  1660   |   0.405347   |     -      |     -     |   6.42   \n",
            "   1    |  1680   |   0.375780   |     -      |     -     |   6.42   \n",
            "   1    |  1700   |   0.428399   |     -      |     -     |   6.42   \n",
            "   1    |  1720   |   0.376237   |     -      |     -     |   6.44   \n",
            "   1    |  1740   |   0.383199   |     -      |     -     |   6.43   \n",
            "   1    |  1760   |   0.368818   |     -      |     -     |   6.43   \n",
            "   1    |  1780   |   0.423701   |     -      |     -     |   6.43   \n",
            "   1    |  1800   |   0.407172   |     -      |     -     |   6.44   \n",
            "   1    |  1820   |   0.408010   |     -      |     -     |   6.41   \n",
            "   1    |  1840   |   0.395948   |     -      |     -     |   6.46   \n",
            "   1    |  1860   |   0.366371   |     -      |     -     |   6.46   \n",
            "   1    |  1880   |   0.378746   |     -      |     -     |   6.42   \n",
            "   1    |  1900   |   0.372273   |     -      |     -     |   6.40   \n",
            "   1    |  1920   |   0.392349   |     -      |     -     |   6.44   \n",
            "   1    |  1940   |   0.391842   |     -      |     -     |   6.44   \n",
            "   1    |  1960   |   0.409138   |     -      |     -     |   6.46   \n",
            "   1    |  1980   |   0.409759   |     -      |     -     |   6.44   \n",
            "   1    |  2000   |   0.399573   |     -      |     -     |   6.40   \n",
            "   1    |  2020   |   0.349620   |     -      |     -     |   6.49   \n",
            "   1    |  2040   |   0.413994   |     -      |     -     |   6.42   \n",
            "   1    |  2060   |   0.371680   |     -      |     -     |   6.42   \n",
            "   1    |  2080   |   0.377585   |     -      |     -     |   6.44   \n",
            "   1    |  2100   |   0.389149   |     -      |     -     |   6.44   \n",
            "   1    |  2120   |   0.425954   |     -      |     -     |   6.46   \n",
            "   1    |  2140   |   0.380364   |     -      |     -     |   6.42   \n",
            "   1    |  2160   |   0.365148   |     -      |     -     |   6.47   \n",
            "   1    |  2180   |   0.395156   |     -      |     -     |   6.45   \n",
            "   1    |  2200   |   0.417695   |     -      |     -     |   6.48   \n",
            "   1    |  2220   |   0.357888   |     -      |     -     |   6.45   \n",
            "   1    |  2240   |   0.415708   |     -      |     -     |   6.45   \n",
            "   1    |  2260   |   0.413319   |     -      |     -     |   6.45   \n",
            "   1    |  2280   |   0.384946   |     -      |     -     |   6.45   \n",
            "   1    |  2300   |   0.386506   |     -      |     -     |   6.46   \n",
            "   1    |  2320   |   0.379225   |     -      |     -     |   6.41   \n",
            "   1    |  2340   |   0.358385   |     -      |     -     |   6.45   \n",
            "   1    |  2360   |   0.392836   |     -      |     -     |   6.44   \n",
            "   1    |  2380   |   0.330857   |     -      |     -     |   6.47   \n",
            "   1    |  2400   |   0.379903   |     -      |     -     |   6.46   \n",
            "   1    |  2420   |   0.382704   |     -      |     -     |   6.40   \n",
            "   1    |  2440   |   0.362045   |     -      |     -     |   6.40   \n",
            "   1    |  2460   |   0.385199   |     -      |     -     |   6.38   \n",
            "   1    |  2480   |   0.349973   |     -      |     -     |   6.41   \n",
            "   1    |  2500   |   0.375971   |     -      |     -     |   6.39   \n",
            "   1    |  2520   |   0.404976   |     -      |     -     |   6.44   \n",
            "   1    |  2540   |   0.375054   |     -      |     -     |   6.43   \n",
            "   1    |  2560   |   0.366719   |     -      |     -     |   6.46   \n",
            "   1    |  2580   |   0.334514   |     -      |     -     |   6.47   \n",
            "   1    |  2600   |   0.400420   |     -      |     -     |   6.44   \n",
            "   1    |  2620   |   0.361372   |     -      |     -     |   6.42   \n",
            "   1    |  2640   |   0.394825   |     -      |     -     |   6.43   \n",
            "   1    |  2660   |   0.369733   |     -      |     -     |   6.42   \n",
            "   1    |  2680   |   0.387509   |     -      |     -     |   6.44   \n",
            "   1    |  2700   |   0.367388   |     -      |     -     |   6.46   \n",
            "   1    |  2720   |   0.356457   |     -      |     -     |   6.44   \n",
            "   1    |  2740   |   0.365465   |     -      |     -     |   6.43   \n",
            "   1    |  2760   |   0.356086   |     -      |     -     |   6.40   \n",
            "   1    |  2780   |   0.379241   |     -      |     -     |   6.41   \n",
            "   1    |  2800   |   0.380450   |     -      |     -     |   6.44   \n",
            "   1    |  2812   |   0.374815   |     -      |     -     |   3.71   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.433296   |  0.358918  |   84.68   |  935.63  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.287815   |     -      |     -     |   6.79   \n",
            "   2    |   40    |   0.294320   |     -      |     -     |   6.40   \n",
            "   2    |   60    |   0.294268   |     -      |     -     |   6.45   \n",
            "   2    |   80    |   0.294678   |     -      |     -     |   6.47   \n",
            "   2    |   100   |   0.322725   |     -      |     -     |   6.41   \n",
            "   2    |   120   |   0.306225   |     -      |     -     |   6.45   \n",
            "   2    |   140   |   0.301220   |     -      |     -     |   6.48   \n",
            "   2    |   160   |   0.347452   |     -      |     -     |   6.45   \n",
            "   2    |   180   |   0.314574   |     -      |     -     |   6.41   \n",
            "   2    |   200   |   0.316431   |     -      |     -     |   6.44   \n",
            "   2    |   220   |   0.331890   |     -      |     -     |   6.43   \n",
            "   2    |   240   |   0.297174   |     -      |     -     |   6.42   \n",
            "   2    |   260   |   0.289060   |     -      |     -     |   6.44   \n",
            "   2    |   280   |   0.323151   |     -      |     -     |   6.43   \n",
            "   2    |   300   |   0.356261   |     -      |     -     |   6.43   \n",
            "   2    |   320   |   0.269367   |     -      |     -     |   6.45   \n",
            "   2    |   340   |   0.311193   |     -      |     -     |   6.46   \n",
            "   2    |   360   |   0.293448   |     -      |     -     |   6.44   \n",
            "   2    |   380   |   0.342824   |     -      |     -     |   6.48   \n",
            "   2    |   400   |   0.301807   |     -      |     -     |   6.45   \n",
            "   2    |   420   |   0.304648   |     -      |     -     |   6.48   \n",
            "   2    |   440   |   0.333418   |     -      |     -     |   6.49   \n",
            "   2    |   460   |   0.290502   |     -      |     -     |   6.47   \n",
            "   2    |   480   |   0.320457   |     -      |     -     |   6.42   \n",
            "   2    |   500   |   0.297750   |     -      |     -     |   6.42   \n",
            "   2    |   520   |   0.333553   |     -      |     -     |   6.41   \n",
            "   2    |   540   |   0.302711   |     -      |     -     |   6.43   \n",
            "   2    |   560   |   0.328381   |     -      |     -     |   6.46   \n",
            "   2    |   580   |   0.335087   |     -      |     -     |   6.42   \n",
            "   2    |   600   |   0.280929   |     -      |     -     |   6.44   \n",
            "   2    |   620   |   0.344722   |     -      |     -     |   6.46   \n",
            "   2    |   640   |   0.364044   |     -      |     -     |   6.44   \n",
            "   2    |   660   |   0.293811   |     -      |     -     |   6.46   \n",
            "   2    |   680   |   0.300085   |     -      |     -     |   6.42   \n",
            "   2    |   700   |   0.340853   |     -      |     -     |   6.43   \n",
            "   2    |   720   |   0.341252   |     -      |     -     |   6.43   \n",
            "   2    |   740   |   0.322287   |     -      |     -     |   6.42   \n",
            "   2    |   760   |   0.290833   |     -      |     -     |   6.45   \n",
            "   2    |   780   |   0.334726   |     -      |     -     |   6.50   \n",
            "   2    |   800   |   0.315786   |     -      |     -     |   6.43   \n",
            "   2    |   820   |   0.316992   |     -      |     -     |   6.46   \n",
            "   2    |   840   |   0.329814   |     -      |     -     |   6.46   \n",
            "   2    |   860   |   0.295175   |     -      |     -     |   6.45   \n",
            "   2    |   880   |   0.328009   |     -      |     -     |   6.44   \n",
            "   2    |   900   |   0.338148   |     -      |     -     |   6.42   \n",
            "   2    |   920   |   0.328584   |     -      |     -     |   6.41   \n",
            "   2    |   940   |   0.276765   |     -      |     -     |   6.47   \n",
            "   2    |   960   |   0.308931   |     -      |     -     |   6.47   \n",
            "   2    |   980   |   0.327324   |     -      |     -     |   6.42   \n",
            "   2    |  1000   |   0.299907   |     -      |     -     |   6.40   \n",
            "   2    |  1020   |   0.332558   |     -      |     -     |   6.48   \n",
            "   2    |  1040   |   0.321201   |     -      |     -     |   6.44   \n",
            "   2    |  1060   |   0.294889   |     -      |     -     |   6.42   \n",
            "   2    |  1080   |   0.324627   |     -      |     -     |   6.46   \n",
            "   2    |  1100   |   0.265700   |     -      |     -     |   6.48   \n",
            "   2    |  1120   |   0.291564   |     -      |     -     |   6.47   \n",
            "   2    |  1140   |   0.342135   |     -      |     -     |   6.50   \n",
            "   2    |  1160   |   0.305611   |     -      |     -     |   6.47   \n",
            "   2    |  1180   |   0.287931   |     -      |     -     |   6.44   \n",
            "   2    |  1200   |   0.344404   |     -      |     -     |   6.45   \n",
            "   2    |  1220   |   0.309781   |     -      |     -     |   6.44   \n",
            "   2    |  1240   |   0.292841   |     -      |     -     |   6.48   \n",
            "   2    |  1260   |   0.316994   |     -      |     -     |   6.47   \n",
            "   2    |  1280   |   0.274824   |     -      |     -     |   6.45   \n",
            "   2    |  1300   |   0.347791   |     -      |     -     |   6.51   \n",
            "   2    |  1320   |   0.327547   |     -      |     -     |   6.48   \n",
            "   2    |  1340   |   0.303698   |     -      |     -     |   6.46   \n",
            "   2    |  1360   |   0.338991   |     -      |     -     |   6.51   \n",
            "   2    |  1380   |   0.315349   |     -      |     -     |   6.44   \n",
            "   2    |  1400   |   0.302058   |     -      |     -     |   6.45   \n",
            "   2    |  1420   |   0.275682   |     -      |     -     |   6.47   \n",
            "   2    |  1440   |   0.311604   |     -      |     -     |   6.51   \n",
            "   2    |  1460   |   0.341760   |     -      |     -     |   6.47   \n",
            "   2    |  1480   |   0.325113   |     -      |     -     |   6.44   \n",
            "   2    |  1500   |   0.293592   |     -      |     -     |   6.42   \n",
            "   2    |  1520   |   0.303445   |     -      |     -     |   6.40   \n",
            "   2    |  1540   |   0.293476   |     -      |     -     |   6.43   \n",
            "   2    |  1560   |   0.344684   |     -      |     -     |   6.46   \n",
            "   2    |  1580   |   0.327423   |     -      |     -     |   6.42   \n",
            "   2    |  1600   |   0.307409   |     -      |     -     |   6.46   \n",
            "   2    |  1620   |   0.308271   |     -      |     -     |   6.44   \n",
            "   2    |  1640   |   0.299951   |     -      |     -     |   6.43   \n",
            "   2    |  1660   |   0.355102   |     -      |     -     |   6.47   \n",
            "   2    |  1680   |   0.267572   |     -      |     -     |   6.46   \n",
            "   2    |  1700   |   0.344331   |     -      |     -     |   6.43   \n",
            "   2    |  1720   |   0.295590   |     -      |     -     |   6.44   \n",
            "   2    |  1740   |   0.340466   |     -      |     -     |   6.41   \n",
            "   2    |  1760   |   0.287662   |     -      |     -     |   6.41   \n",
            "   2    |  1780   |   0.333778   |     -      |     -     |   6.43   \n",
            "   2    |  1800   |   0.308462   |     -      |     -     |   6.46   \n",
            "   2    |  1820   |   0.270424   |     -      |     -     |   6.48   \n",
            "   2    |  1840   |   0.282517   |     -      |     -     |   6.44   \n",
            "   2    |  1860   |   0.314541   |     -      |     -     |   6.45   \n",
            "   2    |  1880   |   0.283722   |     -      |     -     |   6.48   \n",
            "   2    |  1900   |   0.300592   |     -      |     -     |   6.45   \n",
            "   2    |  1920   |   0.310020   |     -      |     -     |   6.45   \n",
            "   2    |  1940   |   0.263026   |     -      |     -     |   6.50   \n",
            "   2    |  1960   |   0.336825   |     -      |     -     |   6.42   \n",
            "   2    |  1980   |   0.288262   |     -      |     -     |   6.45   \n",
            "   2    |  2000   |   0.340230   |     -      |     -     |   6.43   \n",
            "   2    |  2020   |   0.261002   |     -      |     -     |   6.41   \n",
            "   2    |  2040   |   0.291052   |     -      |     -     |   6.46   \n",
            "   2    |  2060   |   0.326938   |     -      |     -     |   6.45   \n",
            "   2    |  2080   |   0.281914   |     -      |     -     |   6.47   \n",
            "   2    |  2100   |   0.318492   |     -      |     -     |   6.45   \n",
            "   2    |  2120   |   0.314370   |     -      |     -     |   6.48   \n",
            "   2    |  2140   |   0.286223   |     -      |     -     |   6.44   \n",
            "   2    |  2160   |   0.363687   |     -      |     -     |   6.46   \n",
            "   2    |  2180   |   0.283370   |     -      |     -     |   6.44   \n",
            "   2    |  2200   |   0.327571   |     -      |     -     |   6.47   \n",
            "   2    |  2220   |   0.292723   |     -      |     -     |   6.45   \n",
            "   2    |  2240   |   0.301681   |     -      |     -     |   6.49   \n",
            "   2    |  2260   |   0.295740   |     -      |     -     |   6.46   \n",
            "   2    |  2280   |   0.315764   |     -      |     -     |   6.42   \n",
            "   2    |  2300   |   0.293774   |     -      |     -     |   6.44   \n",
            "   2    |  2320   |   0.323068   |     -      |     -     |   6.42   \n",
            "   2    |  2340   |   0.336497   |     -      |     -     |   6.44   \n",
            "   2    |  2360   |   0.286263   |     -      |     -     |   6.42   \n",
            "   2    |  2380   |   0.332385   |     -      |     -     |   6.45   \n",
            "   2    |  2400   |   0.241103   |     -      |     -     |   6.44   \n",
            "   2    |  2420   |   0.267678   |     -      |     -     |   6.40   \n",
            "   2    |  2440   |   0.281514   |     -      |     -     |   6.48   \n",
            "   2    |  2460   |   0.291991   |     -      |     -     |   6.45   \n",
            "   2    |  2480   |   0.294556   |     -      |     -     |   6.45   \n",
            "   2    |  2500   |   0.276285   |     -      |     -     |   6.47   \n",
            "   2    |  2520   |   0.321165   |     -      |     -     |   6.45   \n",
            "   2    |  2540   |   0.289762   |     -      |     -     |   6.45   \n",
            "   2    |  2560   |   0.283122   |     -      |     -     |   6.41   \n",
            "   2    |  2580   |   0.316578   |     -      |     -     |   6.45   \n",
            "   2    |  2600   |   0.302857   |     -      |     -     |   6.41   \n",
            "   2    |  2620   |   0.313008   |     -      |     -     |   6.48   \n",
            "   2    |  2640   |   0.303839   |     -      |     -     |   6.45   \n",
            "   2    |  2660   |   0.313347   |     -      |     -     |   6.44   \n",
            "   2    |  2680   |   0.324082   |     -      |     -     |   6.48   \n",
            "   2    |  2700   |   0.271166   |     -      |     -     |   6.44   \n",
            "   2    |  2720   |   0.299698   |     -      |     -     |   6.46   \n",
            "   2    |  2740   |   0.285998   |     -      |     -     |   6.47   \n",
            "   2    |  2760   |   0.297943   |     -      |     -     |   6.48   \n",
            "   2    |  2780   |   0.316225   |     -      |     -     |   6.49   \n",
            "   2    |  2800   |   0.330681   |     -      |     -     |   6.53   \n",
            "   2    |  2812   |   0.316345   |     -      |     -     |   3.78   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.309113   |  0.351026  |   85.12   |  937.84  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.233206   |     -      |     -     |   6.77   \n",
            "   3    |   40    |   0.277692   |     -      |     -     |   6.47   \n",
            "   3    |   60    |   0.243989   |     -      |     -     |   6.51   \n",
            "   3    |   80    |   0.193806   |     -      |     -     |   6.46   \n",
            "   3    |   100   |   0.217662   |     -      |     -     |   6.51   \n",
            "   3    |   120   |   0.223376   |     -      |     -     |   6.45   \n",
            "   3    |   140   |   0.211707   |     -      |     -     |   6.48   \n",
            "   3    |   160   |   0.237141   |     -      |     -     |   6.49   \n",
            "   3    |   180   |   0.224604   |     -      |     -     |   6.46   \n",
            "   3    |   200   |   0.196432   |     -      |     -     |   6.45   \n",
            "   3    |   220   |   0.196868   |     -      |     -     |   6.46   \n",
            "   3    |   240   |   0.210924   |     -      |     -     |   6.48   \n",
            "   3    |   260   |   0.268174   |     -      |     -     |   6.67   \n",
            "   3    |   280   |   0.236839   |     -      |     -     |   6.47   \n",
            "   3    |   300   |   0.253119   |     -      |     -     |   6.48   \n",
            "   3    |   320   |   0.225500   |     -      |     -     |   6.45   \n",
            "   3    |   340   |   0.212343   |     -      |     -     |   6.46   \n",
            "   3    |   360   |   0.234700   |     -      |     -     |   6.43   \n",
            "   3    |   380   |   0.206756   |     -      |     -     |   6.41   \n",
            "   3    |   400   |   0.206068   |     -      |     -     |   6.41   \n",
            "   3    |   420   |   0.219195   |     -      |     -     |   6.44   \n",
            "   3    |   440   |   0.228494   |     -      |     -     |   6.51   \n",
            "   3    |   460   |   0.216212   |     -      |     -     |   6.45   \n",
            "   3    |   480   |   0.204894   |     -      |     -     |   6.41   \n",
            "   3    |   500   |   0.199587   |     -      |     -     |   6.44   \n",
            "   3    |   520   |   0.202842   |     -      |     -     |   6.42   \n",
            "   3    |   540   |   0.228026   |     -      |     -     |   6.42   \n",
            "   3    |   560   |   0.189379   |     -      |     -     |   6.45   \n",
            "   3    |   580   |   0.242170   |     -      |     -     |   6.40   \n",
            "   3    |   600   |   0.221282   |     -      |     -     |   6.41   \n",
            "   3    |   620   |   0.192920   |     -      |     -     |   6.47   \n",
            "   3    |   640   |   0.188997   |     -      |     -     |   6.44   \n",
            "   3    |   660   |   0.251004   |     -      |     -     |   6.45   \n",
            "   3    |   680   |   0.216775   |     -      |     -     |   6.46   \n",
            "   3    |   700   |   0.213197   |     -      |     -     |   6.50   \n",
            "   3    |   720   |   0.224391   |     -      |     -     |   6.44   \n",
            "   3    |   740   |   0.214702   |     -      |     -     |   6.43   \n",
            "   3    |   760   |   0.166839   |     -      |     -     |   6.41   \n",
            "   3    |   780   |   0.180205   |     -      |     -     |   6.44   \n",
            "   3    |   800   |   0.180749   |     -      |     -     |   6.41   \n",
            "   3    |   820   |   0.185257   |     -      |     -     |   6.45   \n",
            "   3    |   840   |   0.236289   |     -      |     -     |   6.45   \n",
            "   3    |   860   |   0.205119   |     -      |     -     |   6.50   \n",
            "   3    |   880   |   0.202357   |     -      |     -     |   6.44   \n",
            "   3    |   900   |   0.177856   |     -      |     -     |   6.50   \n",
            "   3    |   920   |   0.295321   |     -      |     -     |   6.46   \n",
            "   3    |   940   |   0.206428   |     -      |     -     |   6.47   \n",
            "   3    |   960   |   0.191568   |     -      |     -     |   6.42   \n",
            "   3    |   980   |   0.236202   |     -      |     -     |   6.46   \n",
            "   3    |  1000   |   0.208701   |     -      |     -     |   6.45   \n",
            "   3    |  1020   |   0.216436   |     -      |     -     |   6.46   \n",
            "   3    |  1040   |   0.197474   |     -      |     -     |   6.46   \n",
            "   3    |  1060   |   0.200754   |     -      |     -     |   6.44   \n",
            "   3    |  1080   |   0.213214   |     -      |     -     |   6.46   \n",
            "   3    |  1100   |   0.220662   |     -      |     -     |   6.43   \n",
            "   3    |  1120   |   0.158284   |     -      |     -     |   6.47   \n",
            "   3    |  1140   |   0.218754   |     -      |     -     |   6.47   \n",
            "   3    |  1160   |   0.268251   |     -      |     -     |   6.43   \n",
            "   3    |  1180   |   0.192739   |     -      |     -     |   6.44   \n",
            "   3    |  1200   |   0.186119   |     -      |     -     |   6.47   \n",
            "   3    |  1220   |   0.210182   |     -      |     -     |   6.47   \n",
            "   3    |  1240   |   0.206492   |     -      |     -     |   6.43   \n",
            "   3    |  1260   |   0.191226   |     -      |     -     |   6.41   \n",
            "   3    |  1280   |   0.227321   |     -      |     -     |   6.45   \n",
            "   3    |  1300   |   0.235585   |     -      |     -     |   6.46   \n",
            "   3    |  1320   |   0.229906   |     -      |     -     |   6.49   \n",
            "   3    |  1340   |   0.194212   |     -      |     -     |   6.43   \n",
            "   3    |  1360   |   0.209458   |     -      |     -     |   6.45   \n",
            "   3    |  1380   |   0.225590   |     -      |     -     |   6.46   \n",
            "   3    |  1400   |   0.224019   |     -      |     -     |   6.46   \n",
            "   3    |  1420   |   0.185528   |     -      |     -     |   6.40   \n",
            "   3    |  1440   |   0.258599   |     -      |     -     |   6.41   \n",
            "   3    |  1460   |   0.217347   |     -      |     -     |   6.45   \n",
            "   3    |  1480   |   0.277004   |     -      |     -     |   6.45   \n",
            "   3    |  1500   |   0.239314   |     -      |     -     |   6.48   \n",
            "   3    |  1520   |   0.225743   |     -      |     -     |   6.41   \n",
            "   3    |  1540   |   0.225196   |     -      |     -     |   6.40   \n",
            "   3    |  1560   |   0.191134   |     -      |     -     |   6.45   \n",
            "   3    |  1580   |   0.215945   |     -      |     -     |   6.43   \n",
            "   3    |  1600   |   0.208757   |     -      |     -     |   6.40   \n",
            "   3    |  1620   |   0.236036   |     -      |     -     |   6.45   \n",
            "   3    |  1640   |   0.227370   |     -      |     -     |   6.41   \n",
            "   3    |  1660   |   0.201530   |     -      |     -     |   6.44   \n",
            "   3    |  1680   |   0.220854   |     -      |     -     |   6.46   \n",
            "   3    |  1700   |   0.198150   |     -      |     -     |   6.50   \n",
            "   3    |  1720   |   0.188985   |     -      |     -     |   6.50   \n",
            "   3    |  1740   |   0.214450   |     -      |     -     |   6.49   \n",
            "   3    |  1760   |   0.223671   |     -      |     -     |   6.43   \n",
            "   3    |  1780   |   0.259697   |     -      |     -     |   6.46   \n",
            "   3    |  1800   |   0.226969   |     -      |     -     |   6.44   \n",
            "   3    |  1820   |   0.241590   |     -      |     -     |   6.44   \n",
            "   3    |  1840   |   0.238736   |     -      |     -     |   6.48   \n",
            "   3    |  1860   |   0.189605   |     -      |     -     |   6.47   \n",
            "   3    |  1880   |   0.205222   |     -      |     -     |   6.42   \n",
            "   3    |  1900   |   0.240003   |     -      |     -     |   6.44   \n",
            "   3    |  1920   |   0.219710   |     -      |     -     |   6.46   \n",
            "   3    |  1940   |   0.189317   |     -      |     -     |   6.44   \n",
            "   3    |  1960   |   0.199719   |     -      |     -     |   6.45   \n",
            "   3    |  1980   |   0.242329   |     -      |     -     |   6.42   \n",
            "   3    |  2000   |   0.202863   |     -      |     -     |   6.41   \n",
            "   3    |  2020   |   0.239881   |     -      |     -     |   6.46   \n",
            "   3    |  2040   |   0.246801   |     -      |     -     |   6.40   \n",
            "   3    |  2060   |   0.176634   |     -      |     -     |   6.38   \n",
            "   3    |  2080   |   0.194172   |     -      |     -     |   6.46   \n",
            "   3    |  2100   |   0.203864   |     -      |     -     |   6.46   \n",
            "   3    |  2120   |   0.228454   |     -      |     -     |   6.47   \n",
            "   3    |  2140   |   0.225168   |     -      |     -     |   6.46   \n",
            "   3    |  2160   |   0.245563   |     -      |     -     |   6.44   \n",
            "   3    |  2180   |   0.231686   |     -      |     -     |   6.43   \n",
            "   3    |  2200   |   0.274553   |     -      |     -     |   6.45   \n",
            "   3    |  2220   |   0.203612   |     -      |     -     |   6.43   \n",
            "   3    |  2240   |   0.210247   |     -      |     -     |   6.42   \n",
            "   3    |  2260   |   0.249132   |     -      |     -     |   6.41   \n",
            "   3    |  2280   |   0.230533   |     -      |     -     |   6.40   \n",
            "   3    |  2300   |   0.191137   |     -      |     -     |   6.41   \n",
            "   3    |  2320   |   0.205978   |     -      |     -     |   6.42   \n",
            "   3    |  2340   |   0.218544   |     -      |     -     |   6.43   \n",
            "   3    |  2360   |   0.215314   |     -      |     -     |   6.40   \n",
            "   3    |  2380   |   0.238238   |     -      |     -     |   6.40   \n",
            "   3    |  2400   |   0.223837   |     -      |     -     |   6.42   \n",
            "   3    |  2420   |   0.204137   |     -      |     -     |   6.40   \n",
            "   3    |  2440   |   0.220992   |     -      |     -     |   6.42   \n",
            "   3    |  2460   |   0.191002   |     -      |     -     |   6.44   \n",
            "   3    |  2480   |   0.228252   |     -      |     -     |   6.48   \n",
            "   3    |  2500   |   0.174445   |     -      |     -     |   6.43   \n",
            "   3    |  2520   |   0.190969   |     -      |     -     |   6.41   \n",
            "   3    |  2540   |   0.222895   |     -      |     -     |   6.40   \n",
            "   3    |  2560   |   0.204641   |     -      |     -     |   6.49   \n",
            "   3    |  2580   |   0.194253   |     -      |     -     |   6.43   \n",
            "   3    |  2600   |   0.213094   |     -      |     -     |   6.43   \n",
            "   3    |  2620   |   0.187222   |     -      |     -     |   6.44   \n",
            "   3    |  2640   |   0.220851   |     -      |     -     |   6.43   \n",
            "   3    |  2660   |   0.169676   |     -      |     -     |   6.42   \n",
            "   3    |  2680   |   0.236480   |     -      |     -     |   6.48   \n",
            "   3    |  2700   |   0.222522   |     -      |     -     |   6.43   \n",
            "   3    |  2720   |   0.209440   |     -      |     -     |   6.43   \n",
            "   3    |  2740   |   0.213029   |     -      |     -     |   6.44   \n",
            "   3    |  2760   |   0.244598   |     -      |     -     |   6.44   \n",
            "   3    |  2780   |   0.253264   |     -      |     -     |   6.45   \n",
            "   3    |  2800   |   0.205784   |     -      |     -     |   6.45   \n",
            "   3    |  2812   |   0.250709   |     -      |     -     |   3.76   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.216842   |  0.413319  |   84.81   |  937.38  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.166688   |     -      |     -     |   6.78   \n",
            "   4    |   40    |   0.133407   |     -      |     -     |   6.42   \n",
            "   4    |   60    |   0.132674   |     -      |     -     |   6.44   \n",
            "   4    |   80    |   0.125102   |     -      |     -     |   6.45   \n",
            "   4    |   100   |   0.181805   |     -      |     -     |   6.43   \n",
            "   4    |   120   |   0.175462   |     -      |     -     |   6.42   \n",
            "   4    |   140   |   0.111887   |     -      |     -     |   6.40   \n",
            "   4    |   160   |   0.136174   |     -      |     -     |   6.43   \n",
            "   4    |   180   |   0.160085   |     -      |     -     |   6.42   \n",
            "   4    |   200   |   0.154083   |     -      |     -     |   6.43   \n",
            "   4    |   220   |   0.136837   |     -      |     -     |   6.41   \n",
            "   4    |   240   |   0.156080   |     -      |     -     |   6.42   \n",
            "   4    |   260   |   0.140278   |     -      |     -     |   6.49   \n",
            "   4    |   280   |   0.139323   |     -      |     -     |   6.48   \n",
            "   4    |   300   |   0.143923   |     -      |     -     |   6.45   \n",
            "   4    |   320   |   0.120703   |     -      |     -     |   6.43   \n",
            "   4    |   340   |   0.136403   |     -      |     -     |   6.47   \n",
            "   4    |   360   |   0.132128   |     -      |     -     |   6.44   \n",
            "   4    |   380   |   0.182782   |     -      |     -     |   6.45   \n",
            "   4    |   400   |   0.164017   |     -      |     -     |   6.41   \n",
            "   4    |   420   |   0.121653   |     -      |     -     |   6.44   \n",
            "   4    |   440   |   0.161779   |     -      |     -     |   6.41   \n",
            "   4    |   460   |   0.175328   |     -      |     -     |   6.42   \n",
            "   4    |   480   |   0.203986   |     -      |     -     |   6.42   \n",
            "   4    |   500   |   0.122318   |     -      |     -     |   6.43   \n",
            "   4    |   520   |   0.139892   |     -      |     -     |   6.46   \n",
            "   4    |   540   |   0.159591   |     -      |     -     |   6.44   \n",
            "   4    |   560   |   0.132785   |     -      |     -     |   6.48   \n",
            "   4    |   580   |   0.170360   |     -      |     -     |   6.48   \n",
            "   4    |   600   |   0.143204   |     -      |     -     |   6.46   \n",
            "   4    |   620   |   0.163743   |     -      |     -     |   6.44   \n",
            "   4    |   640   |   0.135695   |     -      |     -     |   6.43   \n",
            "   4    |   660   |   0.126116   |     -      |     -     |   6.46   \n",
            "   4    |   680   |   0.128163   |     -      |     -     |   6.45   \n",
            "   4    |   700   |   0.123306   |     -      |     -     |   6.43   \n",
            "   4    |   720   |   0.173155   |     -      |     -     |   6.41   \n",
            "   4    |   740   |   0.183691   |     -      |     -     |   6.45   \n",
            "   4    |   760   |   0.189989   |     -      |     -     |   6.43   \n",
            "   4    |   780   |   0.130317   |     -      |     -     |   6.46   \n",
            "   4    |   800   |   0.142426   |     -      |     -     |   6.51   \n",
            "   4    |   820   |   0.193694   |     -      |     -     |   6.50   \n",
            "   4    |   840   |   0.147696   |     -      |     -     |   6.45   \n",
            "   4    |   860   |   0.160053   |     -      |     -     |   6.44   \n",
            "   4    |   880   |   0.142089   |     -      |     -     |   6.43   \n",
            "   4    |   900   |   0.125052   |     -      |     -     |   6.42   \n",
            "   4    |   920   |   0.134542   |     -      |     -     |   6.43   \n",
            "   4    |   940   |   0.124024   |     -      |     -     |   6.39   \n",
            "   4    |   960   |   0.171749   |     -      |     -     |   6.42   \n",
            "   4    |   980   |   0.138946   |     -      |     -     |   6.45   \n",
            "   4    |  1000   |   0.137086   |     -      |     -     |   6.45   \n",
            "   4    |  1020   |   0.127565   |     -      |     -     |   6.45   \n",
            "   4    |  1040   |   0.195276   |     -      |     -     |   6.45   \n",
            "   4    |  1060   |   0.128958   |     -      |     -     |   6.43   \n",
            "   4    |  1080   |   0.154054   |     -      |     -     |   6.46   \n",
            "   4    |  1100   |   0.139531   |     -      |     -     |   6.46   \n",
            "   4    |  1120   |   0.144603   |     -      |     -     |   6.39   \n",
            "   4    |  1140   |   0.119080   |     -      |     -     |   6.45   \n",
            "   4    |  1160   |   0.131422   |     -      |     -     |   6.44   \n",
            "   4    |  1180   |   0.152017   |     -      |     -     |   6.50   \n",
            "   4    |  1200   |   0.197068   |     -      |     -     |   6.45   \n",
            "   4    |  1220   |   0.168568   |     -      |     -     |   6.46   \n",
            "   4    |  1240   |   0.164053   |     -      |     -     |   6.49   \n",
            "   4    |  1260   |   0.156087   |     -      |     -     |   6.51   \n",
            "   4    |  1280   |   0.150872   |     -      |     -     |   6.49   \n",
            "   4    |  1300   |   0.166977   |     -      |     -     |   6.43   \n",
            "   4    |  1320   |   0.148884   |     -      |     -     |   6.43   \n",
            "   4    |  1340   |   0.176277   |     -      |     -     |   6.46   \n",
            "   4    |  1360   |   0.173222   |     -      |     -     |   6.45   \n",
            "   4    |  1380   |   0.122058   |     -      |     -     |   6.39   \n",
            "   4    |  1400   |   0.192781   |     -      |     -     |   6.43   \n",
            "   4    |  1420   |   0.154769   |     -      |     -     |   6.46   \n",
            "   4    |  1440   |   0.148795   |     -      |     -     |   6.42   \n",
            "   4    |  1460   |   0.176210   |     -      |     -     |   6.45   \n",
            "   4    |  1480   |   0.145568   |     -      |     -     |   6.45   \n",
            "   4    |  1500   |   0.130127   |     -      |     -     |   6.47   \n",
            "   4    |  1520   |   0.142755   |     -      |     -     |   6.49   \n",
            "   4    |  1540   |   0.154670   |     -      |     -     |   6.48   \n",
            "   4    |  1560   |   0.123061   |     -      |     -     |   6.44   \n",
            "   4    |  1580   |   0.119192   |     -      |     -     |   6.43   \n",
            "   4    |  1600   |   0.148658   |     -      |     -     |   6.45   \n",
            "   4    |  1620   |   0.161801   |     -      |     -     |   6.43   \n",
            "   4    |  1640   |   0.105601   |     -      |     -     |   6.49   \n",
            "   4    |  1660   |   0.172060   |     -      |     -     |   6.42   \n",
            "   4    |  1680   |   0.132451   |     -      |     -     |   6.48   \n",
            "   4    |  1700   |   0.135302   |     -      |     -     |   6.45   \n",
            "   4    |  1720   |   0.138870   |     -      |     -     |   6.43   \n",
            "   4    |  1740   |   0.162592   |     -      |     -     |   6.47   \n",
            "   4    |  1760   |   0.151008   |     -      |     -     |   6.48   \n",
            "   4    |  1780   |   0.155096   |     -      |     -     |   6.45   \n",
            "   4    |  1800   |   0.212767   |     -      |     -     |   6.46   \n",
            "   4    |  1820   |   0.164099   |     -      |     -     |   6.46   \n",
            "   4    |  1840   |   0.131233   |     -      |     -     |   6.48   \n",
            "   4    |  1860   |   0.143503   |     -      |     -     |   6.49   \n",
            "   4    |  1880   |   0.153925   |     -      |     -     |   6.45   \n",
            "   4    |  1900   |   0.167027   |     -      |     -     |   6.46   \n",
            "   4    |  1920   |   0.204267   |     -      |     -     |   6.46   \n",
            "   4    |  1940   |   0.214991   |     -      |     -     |   6.44   \n",
            "   4    |  1960   |   0.142595   |     -      |     -     |   6.46   \n",
            "   4    |  1980   |   0.139534   |     -      |     -     |   6.43   \n",
            "   4    |  2000   |   0.156638   |     -      |     -     |   6.40   \n",
            "   4    |  2020   |   0.172515   |     -      |     -     |   6.44   \n",
            "   4    |  2040   |   0.166868   |     -      |     -     |   6.41   \n",
            "   4    |  2060   |   0.128103   |     -      |     -     |   6.42   \n",
            "   4    |  2080   |   0.158252   |     -      |     -     |   6.43   \n",
            "   4    |  2100   |   0.162126   |     -      |     -     |   6.46   \n",
            "   4    |  2120   |   0.188335   |     -      |     -     |   6.43   \n",
            "   4    |  2140   |   0.136608   |     -      |     -     |   6.46   \n",
            "   4    |  2160   |   0.136852   |     -      |     -     |   6.44   \n",
            "   4    |  2180   |   0.113626   |     -      |     -     |   6.43   \n",
            "   4    |  2200   |   0.167449   |     -      |     -     |   6.48   \n",
            "   4    |  2220   |   0.198011   |     -      |     -     |   6.41   \n",
            "   4    |  2240   |   0.127346   |     -      |     -     |   6.41   \n",
            "   4    |  2260   |   0.163270   |     -      |     -     |   6.43   \n",
            "   4    |  2280   |   0.148238   |     -      |     -     |   6.42   \n",
            "   4    |  2300   |   0.125597   |     -      |     -     |   6.45   \n",
            "   4    |  2320   |   0.127533   |     -      |     -     |   6.47   \n",
            "   4    |  2340   |   0.143971   |     -      |     -     |   6.46   \n",
            "   4    |  2360   |   0.161055   |     -      |     -     |   6.44   \n",
            "   4    |  2380   |   0.124183   |     -      |     -     |   6.48   \n",
            "   4    |  2400   |   0.151185   |     -      |     -     |   6.49   \n",
            "   4    |  2420   |   0.139463   |     -      |     -     |   6.43   \n",
            "   4    |  2440   |   0.153491   |     -      |     -     |   6.51   \n",
            "   4    |  2460   |   0.185648   |     -      |     -     |   6.43   \n",
            "   4    |  2480   |   0.191393   |     -      |     -     |   6.45   \n",
            "   4    |  2500   |   0.111556   |     -      |     -     |   6.46   \n",
            "   4    |  2520   |   0.207644   |     -      |     -     |   6.49   \n",
            "   4    |  2540   |   0.136739   |     -      |     -     |   6.50   \n",
            "   4    |  2560   |   0.174850   |     -      |     -     |   6.48   \n",
            "   4    |  2580   |   0.112910   |     -      |     -     |   6.49   \n",
            "   4    |  2600   |   0.151672   |     -      |     -     |   6.47   \n",
            "   4    |  2620   |   0.165242   |     -      |     -     |   6.50   \n",
            "   4    |  2640   |   0.125449   |     -      |     -     |   6.47   \n",
            "   4    |  2660   |   0.162436   |     -      |     -     |   6.46   \n",
            "   4    |  2680   |   0.092236   |     -      |     -     |   6.46   \n",
            "   4    |  2700   |   0.103700   |     -      |     -     |   6.47   \n",
            "   4    |  2720   |   0.129014   |     -      |     -     |   6.47   \n",
            "   4    |  2740   |   0.173119   |     -      |     -     |   6.46   \n",
            "   4    |  2760   |   0.119918   |     -      |     -     |   6.45   \n",
            "   4    |  2780   |   0.133054   |     -      |     -     |   6.43   \n",
            "   4    |  2800   |   0.209022   |     -      |     -     |   6.41   \n",
            "   4    |  2812   |   0.155392   |     -      |     -     |   3.71   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.150814   |  0.539382  |   84.34   |  937.67  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbwG2akMU5GM"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        \n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "     \n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    \n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    \n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOSXTjkkVBK_"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "    \n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "VWJ6zMe-VQbs",
        "outputId": "b6ff162a-644c-4e03-b1f3-8fce8cc7621c"
      },
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "evaluate_roc(probs, y_val)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.9194\n",
            "Accuracy: 84.32%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1cMpgwU7qKC"
      },
      "source": [
        "##Predict on trump tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FszCSBMYIaq_"
      },
      "source": [
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/trump_tweets_v3.csv\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "C_Y1hx-PJXwe",
        "outputId": "ed946a28-1cc6-40e8-eb66-0b2a8ba1e991"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Date</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I was thrilled to be back in the Great city of...</td>\n",
              "      <td>2020-03-03 01:34:50</td>\n",
              "      <td>0.483333</td>\n",
              "      <td>0.458929</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>READ: Letter to surveillance court obtained by...</td>\n",
              "      <td>2020-01-17 03:22:47</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Unsolicited Mail In Ballot Scam is a major...</td>\n",
              "      <td>2020-09-12 20:10:58</td>\n",
              "      <td>0.454762</td>\n",
              "      <td>0.021131</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Very friendly telling of events here about Com...</td>\n",
              "      <td>2020-01-17 13:13:59</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.268750</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>President  announced historic steps to protect...</td>\n",
              "      <td>2020-01-17 00:11:56</td>\n",
              "      <td>0.200794</td>\n",
              "      <td>0.095238</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Tweet  ... Sentiment\n",
              "0  I was thrilled to be back in the Great city of...  ...  Positive\n",
              "1  READ: Letter to surveillance court obtained by...  ...  Positive\n",
              "2  The Unsolicited Mail In Ballot Scam is a major...  ...  Positive\n",
              "3  Very friendly telling of events here about Com...  ...  Positive\n",
              "4  President  announced historic steps to protect...  ...  Positive\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuSB2DC0Yauz",
        "outputId": "fd34ea6c-0964-41e4-e020-6727f3e093d8"
      },
      "source": [
        "##Predict on Test set\r\n",
        "## Preprocess test dataset\r\n",
        "print('Start of preprocessing...')\r\n",
        "test_inputs, test_masks = preprocessing_for_bert(test_data.Tweet)\r\n",
        "\r\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\r\n",
        "test_sampler = SequentialSampler(test_dataset)\r\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of preprocessing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LquciBs97pSu"
      },
      "source": [
        "probs = bert_predict(bert_classifier, test_dataloader)\r\n",
        "\r\n",
        "threshold = 0.5\r\n",
        "preds = np.where(probs[:, 1] >= threshold, 1, 0)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eINOXy2KNm0",
        "outputId": "21c4c9d6-73eb-4959-d776-af0c8989037a"
      },
      "source": [
        "output = test_data[preds==1]\r\n",
        "print(\"Positive Tweets:\")\r\n",
        "list(output.sample(10).Tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Tweets:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A GREAT Senator. Must return to Washington. Steve has my Complete &amp; Total Endorsement! ',\n",
              " 'Congratulations to Dustin Johnson  on not only a great winning streak and golf season, but on capping it off with a fantastic  Tour Championship and becoming the 2020 FEDEXCUP Champion. Dustin is a true WINNER in so many ways!',\n",
              " 'Great honor, but think we are much higher than 46%. Hidden vote! ',\n",
              " 'Will be landing shortly in the Great State of North Carolina. See you soon!',\n",
              " 'Barry Moore () will be a terrific Congressman for Alabama! An early supporter of our MAGA agenda, he is Strong on Jobs, Life, the Wall, Law &amp; Order and the Second Amendment. Barry has my Complete and Total Endorsement! AL02 ',\n",
              " 'Today, President  signed into the law the CARESAct which will help small businesses stay open, help workerâ\\x80¦',\n",
              " 'We are living a GREAT AMERICAN STORYâ\\x80\\x94only because you are in it, President !\\n\\nâ\\x80\\x9cEvery time I think of you,â\\x80¦',\n",
              " 'I am pleased to announce the nomination of  (Congressman John Ratcliffe) to be Director of National Intelligence (DNI). Would have completed process earlier, but John wanted to wait until after IG Report was finished. John is an outstanding man of great talent!',\n",
              " '\"Today, I\\'m announcing a surge of federal law enforcement into American communities plagued by violent crime. We\\'ll work evâ\\x80¦',\n",
              " 'The greatest Election Fraud in our history is about to happen. This may top the Democrats illegally spying on my campaign! ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7euIkJgKbes",
        "outputId": "3af87d3d-f171-458f-ddae-1c4f6223df2d"
      },
      "source": [
        "output = test_data[preds==0]\r\n",
        "print(\"Sample Negative Tweets:\")\r\n",
        "list(output.sample(10).Tweet)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Negative Tweets:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['., a great American, is hospitalized with coronavirus and has requested prayer. He is a colon cancer surâ\\x80¦',\n",
              " 'What is the purpose of having White House News Conferences when the Lamestream Media asks nothing but hostile questions, &amp; then refuses to report the truth or facts accurately. They get record ratings, &amp; the American people get nothing but Fake News. Not worth the time &amp; effort!',\n",
              " 'Seattle Looters, Agitators, Anarchists and â\\x80\\x9cProtestorsâ\\x80\\x9d, are now refusing to leave the â\\x80\\x9cCHOPâ\\x80\\x9d Zone. They have ZERO respect for Government, or the Mayor of Seattle or Governor of Washington State! Not good!',\n",
              " '. Poll â\\x80\\x9cGives President Trump a 52% Approval Rating in North Carolina, and a seven point lead over (Sleepy) Joe Biden. The President also helps other Republican Candidates, including , who has a 4% lead over his Democrat rival.â\\x80\\x9d',\n",
              " '. &amp; third place anchor, , are doing everything in their power to demean our Country, much to the benefit of the Radical Left Democrats. Tonight they put on yet another Fake â\\x80\\x9cWhistleblowerâ\\x80\\x9d, a disgruntled employee who supports Dems, fabricates stories, &amp;...',\n",
              " 'President Trump talks about all the things we can do\\n\\nJoe Biden talks about everything we *can\\'t* do\\n\\n\"It\\'s very defeatist. It\\'â\\x80¦',\n",
              " 'The Do Nothing Democrats and their leader, the Fake News Lamestream Media, are doing everything possible to hurt and disparage our Country. No matter what we do or say, no matter how big a win, they report that it was a loss, or not good enough. The Enemy of the People!',\n",
              " 'Sleepy Joe Biden and the Radical Left Democrats want to â\\x80\\x9cDEFUND THE POLICEâ\\x80\\x9d. I want great and well paid LAW ENFORCEMENT. I want LAW &amp; ORDER!',\n",
              " 'Fake News CNN is at it again, carrying water for Joe Biden!  \\n',\n",
              " 'Our farmers and ranchers are instrumental to Oklahomaâ\\x80\\x99s economy and theyâ\\x80\\x99ve been hit hard by COVID19. This funding will briâ\\x80¦']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPxBJJoXR902",
        "outputId": "bd4c3267-be26-4740-c821-81a0ab4e14be"
      },
      "source": [
        "test_data.shape[0]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5943"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRcPfFA_LFAN",
        "outputId": "8cd6e681-055f-427e-a12a-e95a7ea4dc5e"
      },
      "source": [
        "positivePred = test_data[preds==1]\r\n",
        "negativePred = test_data[preds==0]\r\n",
        "print(str(positivePred.shape[0]/(test_data.shape[0])*100) + \" % of positive tweets\")\r\n",
        "print(str(negativePred.shape[0]/(test_data.shape[0])*100) + \" % of negative tweets\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58.40484603735487 % of positive tweets\n",
            "41.59515396264513 % of negative tweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "eS4kmg9htK8e",
        "outputId": "3de2c78d-fff5-4570-c519-b678cb697c3c"
      },
      "source": [
        "test_data[preds==1].head(20)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Date</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I was thrilled to be back in the Great city of...</td>\n",
              "      <td>2020-03-03 01:34:50</td>\n",
              "      <td>0.483333</td>\n",
              "      <td>0.458929</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Very friendly telling of events here about Com...</td>\n",
              "      <td>2020-01-17 13:13:59</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.268750</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>President  announced historic steps to protect...</td>\n",
              "      <td>2020-01-17 00:11:56</td>\n",
              "      <td>0.200794</td>\n",
              "      <td>0.095238</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Getting a little exercise this morning!</td>\n",
              "      <td>2020-02-01 16:14:02</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>-0.234375</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>$13.9M is heading to New Orleans in  funding f...</td>\n",
              "      <td>2020-08-12 18:43:05</td>\n",
              "      <td>0.640909</td>\n",
              "      <td>0.466591</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Tonight at 9:00 P.M. Enjoy!</td>\n",
              "      <td>2020-09-15 23:16:49</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>...Based on decisions being rendered now, this...</td>\n",
              "      <td>2020-06-18 17:24:18</td>\n",
              "      <td>0.508929</td>\n",
              "      <td>0.314286</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>BIG VICTORY for patients â Federal court UPH...</td>\n",
              "      <td>2020-06-24 01:21:21</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Rushing him out of basement after seeing some ...</td>\n",
              "      <td>2020-08-30 22:16:33</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>-0.325000</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>HAPPY EASTER!</td>\n",
              "      <td>2020-04-12 14:21:55</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Much more âdisinformationâ coming out of C...</td>\n",
              "      <td>2020-05-30 18:33:36</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>-0.037500</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>NEW RECORD FOR NASDAQ!</td>\n",
              "      <td>2020-08-17 20:50:15</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.170455</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>So great for the World!</td>\n",
              "      <td>2020-09-20 12:49:25</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Great story. Thank you to Mr. Young of Jonesbo...</td>\n",
              "      <td>2020-03-21 13:21:10</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.462500</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>LIVE: President  holds a news conference</td>\n",
              "      <td>2020-09-27 21:32:12</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Nearly 4 years ago President  told America and...</td>\n",
              "      <td>2020-05-31 02:27:36</td>\n",
              "      <td>0.427273</td>\n",
              "      <td>0.118182</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Today is the culmination of three and a half y...</td>\n",
              "      <td>2020-05-31 02:27:36</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>-0.166667</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Great book!</td>\n",
              "      <td>2020-09-28 04:29:21</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>President : The 20 most dangerous American cit...</td>\n",
              "      <td>2020-06-25 04:28:27</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>-0.033333</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Keep it up . We've got a long way to go with t...</td>\n",
              "      <td>2020-09-01 03:45:16</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Tweet  ... Sentiment\n",
              "0   I was thrilled to be back in the Great city of...  ...  Positive\n",
              "3   Very friendly telling of events here about Com...  ...  Positive\n",
              "4   President  announced historic steps to protect...  ...  Positive\n",
              "5            Getting a little exercise this morning!   ...  Negative\n",
              "8   $13.9M is heading to New Orleans in  funding f...  ...  Positive\n",
              "9                        Tonight at 9:00 P.M. Enjoy!   ...  Positive\n",
              "11  ...Based on decisions being rendered now, this...  ...  Positive\n",
              "13  BIG VICTORY for patients â Federal court UPH...  ...  Positive\n",
              "16  Rushing him out of basement after seeing some ...  ...  Negative\n",
              "18                                      HAPPY EASTER!  ...  Positive\n",
              "19  Much more âdisinformationâ coming out of C...  ...  Negative\n",
              "20                             NEW RECORD FOR NASDAQ!  ...  Positive\n",
              "22                           So great for the World!   ...  Positive\n",
              "23  Great story. Thank you to Mr. Young of Jonesbo...  ...  Positive\n",
              "25          LIVE: President  holds a news conference   ...  Positive\n",
              "26  Nearly 4 years ago President  told America and...  ...  Positive\n",
              "27  Today is the culmination of three and a half y...  ...  Negative\n",
              "28                                       Great book!   ...  Positive\n",
              "29  President : The 20 most dangerous American cit...  ...  Negative\n",
              "33  Keep it up . We've got a long way to go with t...  ...  Positive\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}