{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60e4b8cae1484ecb9c706f04ab5078d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c10f29d943794b28a15f70b422d43b95",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5d30a60c10364bf9bcb73ef9df79f119",
              "IPY_MODEL_a7d1d171b98b4b2ca58dffd7b0fa517e"
            ]
          }
        },
        "c10f29d943794b28a15f70b422d43b95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d30a60c10364bf9bcb73ef9df79f119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_43adad41ea2946dbb5e40ad155b20daa",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d547bf5a34c04758b75b79abe99d9d2d"
          }
        },
        "a7d1d171b98b4b2ca58dffd7b0fa517e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0838c8169ec34dcda3b984a91242b18b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 2.16MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fcd8d318f7d54534b65d720d3e73e4fe"
          }
        },
        "43adad41ea2946dbb5e40ad155b20daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d547bf5a34c04758b75b79abe99d9d2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0838c8169ec34dcda3b984a91242b18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fcd8d318f7d54534b65d720d3e73e4fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcPJdwHXmlrY",
        "outputId": "e192eb95-fcd7-48e3-daf9-020e81b388cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 17.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=fe6bb33a592bac1d73167cf9e8ffc1f1139a55c87e820b5c381ede06f31689ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4qLcS4nxIL"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hLnPpV5pLzJ"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRCKvcB1pmgs"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Data/training.1600000.processed.noemoticon.csv\", names=['target', 'ids', 'date', 'flag', 'user', 'text'],\n",
        "                encoding='latin-1', error_bad_lines=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuQ7vtZRpq98",
        "outputId": "14321cd0-7df4-4037-c7d3-9bfd8bd2d487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  ...                                               text\n",
              "0       0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1       0  ...  is upset that he can't update his Facebook by ...\n",
              "2       0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3       0  ...    my whole body feels itchy and like its on fire \n",
              "4       0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxhZxeYxsMrQ"
      },
      "source": [
        "down_sample = 10000\n",
        "neg_index = np.random.choice(dataset[dataset.target == 0].index, size=int(.5*down_sample), replace=False)\n",
        "pos_index = np.random.choice(dataset[dataset.target == 4].index, size=int(.5*down_sample), replace=False)\n",
        "red_index = np.concatenate((neg_index, pos_index))\n",
        "data = dataset.iloc[red_index,:]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRZPOfr9kUoK",
        "outputId": "18324f8f-2bf4-4987-8fd5-f4f195b0a1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>267825</th>\n",
              "      <td>0</td>\n",
              "      <td>1989272908</td>\n",
              "      <td>Mon Jun 01 00:57:08 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>juicyjanice</td>\n",
              "      <td>this sleepyhead is not cooperating well today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77227</th>\n",
              "      <td>0</td>\n",
              "      <td>1696081246</td>\n",
              "      <td>Mon May 04 07:34:16 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>RobinMoberg</td>\n",
              "      <td>leaving orlando today  i'm going to miss it!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765108</th>\n",
              "      <td>0</td>\n",
              "      <td>2299269756</td>\n",
              "      <td>Tue Jun 23 12:42:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>nofxjunkie</td>\n",
              "      <td>i am most excited about cutting up dead people...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424044</th>\n",
              "      <td>0</td>\n",
              "      <td>2063005449</td>\n",
              "      <td>Sun Jun 07 01:02:15 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>helen_lo</td>\n",
              "      <td>@Ozquilter Back home now.  Both there to greet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>729818</th>\n",
              "      <td>0</td>\n",
              "      <td>2263405239</td>\n",
              "      <td>Sun Jun 21 00:43:35 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>almoyo</td>\n",
              "      <td>The escalator ate my dress. I really liked thi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        target  ...                                               text\n",
              "267825       0  ...     this sleepyhead is not cooperating well today \n",
              "77227        0  ...      leaving orlando today  i'm going to miss it!!\n",
              "765108       0  ...  i am most excited about cutting up dead people...\n",
              "424044       0  ...  @Ozquilter Back home now.  Both there to greet...\n",
              "729818       0  ...  The escalator ate my dress. I really liked thi...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gr0MWQMp5IE",
        "outputId": "ff6d5267-1447-4aa2-ac29-a7859d06c5fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data.target = data.target.replace({0: 0, 4: 1})\n",
        "data.target.value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5168: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    5000\n",
              "0    5000\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg-8ZdwJp91g"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.text.values\n",
        "y = data.target.values\n",
        "\n",
        "X_train, X_val, y_train, y_val =\\\n",
        "    train_test_split(X, y, test_size=0.1, random_state=2020)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9iCQsoArROb"
      },
      "source": [
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgkQ91srreJk",
        "outputId": "d39ec388-1b5d-4409-caee-5703d1c896aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "60e4b8cae1484ecb9c706f04ab5078d5",
            "c10f29d943794b28a15f70b422d43b95",
            "5d30a60c10364bf9bcb73ef9df79f119",
            "a7d1d171b98b4b2ca58dffd7b0fa517e",
            "43adad41ea2946dbb5e40ad155b20daa",
            "d547bf5a34c04758b75b79abe99d9d2d",
            "0838c8169ec34dcda3b984a91242b18b",
            "fcd8d318f7d54534b65d720d3e73e4fe"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60e4b8cae1484ecb9c706f04ab5078d5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00D0fFuty_iq",
        "outputId": "d79b083a-4cdc-40bd-de25-1448357406f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "all_tweets = data.text.values\n",
        "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True, max_length=510) for sent in all_tweets]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1NcFnbW0M9R",
        "outputId": "742e9591-673d-476b-91d9-744595a28fb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "max_len = max([len(sent) for sent in encoded_tweets])\n",
        "print('Max length: ', max_len)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length:  141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DabdUce30Oo",
        "outputId": "e9f8797a-4bbb-4a50-eade-20a93b9c40cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN = 141\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  this sleepyhead is not cooperating well today \n",
            "Token IDs:  [101, 2023, 17056, 4974, 2003, 2025, 6201, 5844, 2092, 2651, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Tokenizing data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWyvvJCvI7g",
        "outputId": "3893b4ed-7a23-43aa-8c13-8fddfb452c34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgtCK8OB-lBK",
        "outputId": "3563d59f-c13f-45af-cab0-5b703ebedc7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "print(\"Shape of train_dataloader\", len(train_dataloader))\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "print(\"Shape of val_dataloader\", len(val_dataloader))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_dataloader 282\n",
            "Shape of val_dataloader 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F11o1rbCBiVK",
        "outputId": "bac1d7b3-3543-4506-bb41-3cc42ce95b41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "#import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 42 µs, sys: 0 ns, total: 42 µs\n",
            "Wall time: 44.8 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP2r78mtCojU"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(\"total_steps\", total_steps)\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM4NFERfC7HR"
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    with torch.no_grad():\n",
        "      for batch in val_dataloader:\n",
        "          # Load batch to GPU\n",
        "          b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "          # Compute logits\n",
        "          \n",
        "          logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "          # Compute loss\n",
        "          loss = loss_fn(logits, b_labels)\n",
        "          val_loss.append(loss.item())\n",
        "\n",
        "          # Get the predictions\n",
        "          preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "          # Calculate the accuracy rate\n",
        "          accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "          val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhukZRn4DKyg",
        "outputId": "5592ed72-6631-4e86-ef39-69904598576f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=3)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=3, evaluation=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total_steps 846\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.654539   |     -      |     -     |   15.78  \n",
            "   1    |   40    |   0.558545   |     -      |     -     |   15.39  \n",
            "   1    |   60    |   0.562254   |     -      |     -     |   15.54  \n",
            "   1    |   80    |   0.529388   |     -      |     -     |   15.27  \n",
            "   1    |   100   |   0.484347   |     -      |     -     |   15.07  \n",
            "   1    |   120   |   0.443520   |     -      |     -     |   15.07  \n",
            "   1    |   140   |   0.461330   |     -      |     -     |   15.17  \n",
            "   1    |   160   |   0.424560   |     -      |     -     |   15.28  \n",
            "   1    |   180   |   0.426573   |     -      |     -     |   15.29  \n",
            "   1    |   200   |   0.409384   |     -      |     -     |   15.27  \n",
            "   1    |   220   |   0.463938   |     -      |     -     |   15.15  \n",
            "   1    |   240   |   0.422263   |     -      |     -     |   15.16  \n",
            "   1    |   260   |   0.419167   |     -      |     -     |   15.22  \n",
            "   1    |   280   |   0.427329   |     -      |     -     |   15.27  \n",
            "   1    |   281   |   0.313567   |     -      |     -     |   0.24   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.477698   |  0.400939  |   82.23   |  222.52  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.278236   |     -      |     -     |   15.96  \n",
            "   2    |   40    |   0.296861   |     -      |     -     |   15.19  \n",
            "   2    |   60    |   0.309724   |     -      |     -     |   15.19  \n",
            "   2    |   80    |   0.243376   |     -      |     -     |   15.16  \n",
            "   2    |   100   |   0.282484   |     -      |     -     |   15.17  \n",
            "   2    |   120   |   0.254389   |     -      |     -     |   15.20  \n",
            "   2    |   140   |   0.290863   |     -      |     -     |   15.22  \n",
            "   2    |   160   |   0.307550   |     -      |     -     |   15.23  \n",
            "   2    |   180   |   0.250900   |     -      |     -     |   15.26  \n",
            "   2    |   200   |   0.304137   |     -      |     -     |   15.23  \n",
            "   2    |   220   |   0.290999   |     -      |     -     |   15.22  \n",
            "   2    |   240   |   0.319903   |     -      |     -     |   15.20  \n",
            "   2    |   260   |   0.303853   |     -      |     -     |   15.20  \n",
            "   2    |   280   |   0.254339   |     -      |     -     |   15.19  \n",
            "   2    |   281   |   0.125894   |     -      |     -     |   0.25   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.284243   |  0.466094  |   81.45   |  222.20  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.184503   |     -      |     -     |   15.92  \n",
            "   3    |   40    |   0.180952   |     -      |     -     |   15.21  \n",
            "   3    |   60    |   0.174267   |     -      |     -     |   15.20  \n",
            "   3    |   80    |   0.154829   |     -      |     -     |   15.20  \n",
            "   3    |   100   |   0.150575   |     -      |     -     |   15.21  \n",
            "   3    |   120   |   0.118430   |     -      |     -     |   15.18  \n",
            "   3    |   140   |   0.161313   |     -      |     -     |   15.19  \n",
            "   3    |   160   |   0.162927   |     -      |     -     |   15.17  \n",
            "   3    |   180   |   0.106653   |     -      |     -     |   15.20  \n",
            "   3    |   200   |   0.150531   |     -      |     -     |   15.18  \n",
            "   3    |   220   |   0.153130   |     -      |     -     |   15.21  \n",
            "   3    |   240   |   0.161223   |     -      |     -     |   15.21  \n",
            "   3    |   260   |   0.134454   |     -      |     -     |   15.19  \n",
            "   3    |   280   |   0.209099   |     -      |     -     |   15.20  \n",
            "   3    |   281   |   0.300952   |     -      |     -     |   0.24   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.157955   |  0.615287  |   81.93   |  222.06  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbwG2akMU5GM"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        \n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "     \n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    \n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    \n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOSXTjkkVBK_"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "    \n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWJ6zMe-VQbs",
        "outputId": "f7386f0d-b78d-4a92-88f8-2f5868dd3199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "evaluate_roc(probs, y_val)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.9024\n",
            "Accuracy: 81.80%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuSB2DC0Yauz"
      },
      "source": [
        "##Predict on Test set"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}